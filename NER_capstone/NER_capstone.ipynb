{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "capstone_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EuwwBk5bnFkK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0c4ba3c-a528-4aed-d40d-1c8bb7c6592d"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.util import ngrams\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "random.seed(1024)\n",
        "FloatTensor = torch.cuda.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor\n",
        "ByteTensor = torch.cuda.ByteTensor\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6hC64Rv-Qm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "! pip install mlflow\n",
        "! pip install sklearn-crfsuite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCjDOwPXvf7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "79f09e41-bffc-402a-a055-c7061883cd4c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q67If6EOQ5wi",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Wc39ikuy7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4bf022cf-0521-451a-fa32-c34a750231ec"
      },
      "source": [
        "new_text = []\n",
        "path = 'drive/My Drive/AISC/nlp/data/train/'\n",
        "\n",
        "for file in os.listdir(path)[:100]:\n",
        "    file = open(path+file, encoding='UTF-8')  \n",
        "    text = file.readlines()\n",
        "    for i in text:\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        i = list(filter(None, i.strip('\\n').strip('\\n').strip(' ').split(' ')))\n",
        "        for j in i:\n",
        "            tokens.append(j[:j.find('[')])\n",
        "            labels.append(j[j.find('['):])\n",
        "        new_text.append([tokens, labels])\n",
        "      \n",
        "print(len(new_text), 'sentences after combining documents')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32253 sentences after combining documents\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcA_S7wAWlDE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "106441a3-e8fe-4214-c6f2-09b9fa4dcb38"
      },
      "source": [
        "x, y = list(zip(*new_text))\n",
        "vocab = list(set([item for sublist in x for item in list(filter(None, sublist))]))\n",
        "tags = list(set([item for sublist in y for item in list(filter(None, sublist))]))\n",
        "\n",
        "# label counts\n",
        "# Notice the skew in the dataset. We will need to remove the [0] label at the end\n",
        "pd.Series([item for sublist in y for item in list(filter(None, sublist))]).value_counts()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]      1280910\n",
              "[LEG]       1385\n",
              "[CNP]       1157\n",
              "[GOV]        632\n",
              "[TIT]        631\n",
              "[STD]        445\n",
              "[JUR]        438\n",
              "[EFD]        114\n",
              "[VAL]         99\n",
              "[TED]         95\n",
              "[PER]         58\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kGUH2jAV2ac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60dd69fe-41a4-4e25-ddb8-28a88e99f44c"
      },
      "source": [
        "word2index={'<UNK>' : 0, '<DUMMY>' : 1} # dummy token is for start or end of sentence\n",
        "\n",
        "for vo in vocab:\n",
        "    if word2index.get(vo) is None:\n",
        "        word2index[vo] = len(word2index)\n",
        "\n",
        "index2word = {v:k for k, v in word2index.items()}\n",
        "\n",
        "tag2index = {}\n",
        "for tag in tags:\n",
        "    if tag2index.get(tag) is None:\n",
        "        tag2index[tag] = len(tag2index) \n",
        "index2tag={v:k for k, v in tag2index.items()}\n",
        "\n",
        "print(len(word2index), 'unique words in text')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26704 unique words in text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DehNoKWNGfPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "badea729-2043-4038-a818-b036fe6378f3"
      },
      "source": [
        "WINDOW_SIZE = 2\n",
        "windows = []\n",
        "\n",
        "for sample in new_text:\n",
        "    #print(sample)\n",
        "    dummy = ['<DUMMY>'] * WINDOW_SIZE\n",
        "    window = list(nltk.ngrams(dummy + list(sample[0]) + dummy, WINDOW_SIZE * 2 + 1))\n",
        "    windows.extend([[list(window[i]), sample[1][i]] for i in range(len(sample[0]))])\n",
        "windows[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<DUMMY>', '<DUMMY>', 'TOKEN_57', 'TOKEN_2368', 'TOKEN_61'], '[0]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FivaVZ5Q8fY",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "455hg55d5IYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WindowClassifier(nn.Module): \n",
        "    def __init__(self, vocab_size, embedding_size, window_size, hidden_size, output_size):\n",
        "\n",
        "        super(WindowClassifier, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.h_layer1 = nn.Linear(embedding_size * (window_size * 2 + 1), hidden_size)\n",
        "        self.h_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.o_layer = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, inputs, is_training=False): \n",
        "        embeds = self.embed(inputs) # BxWxD\n",
        "        concated = embeds.view(-1, embeds.size(1)*embeds.size(2)) # Bx(W*D)\n",
        "        h0 = self.relu(self.h_layer1(concated))\n",
        "        if is_training:\n",
        "            h0 = self.dropout(h0)\n",
        "        h1 = self.relu(self.h_layer2(h0))\n",
        "        if is_training:\n",
        "            h1 = self.dropout(h1)\n",
        "        out = self.softmax(self.o_layer(h1))\n",
        "        return out\n",
        "\n",
        "def getBatch(batch_size, train_data):\n",
        "    random.shuffle(train_data)\n",
        "    sindex = 0\n",
        "    eindex = batch_size\n",
        "    while eindex < len(train_data):\n",
        "        batch = train_data[sindex: eindex]\n",
        "        temp = eindex\n",
        "        eindex = eindex + batch_size\n",
        "        sindex = temp\n",
        "        yield batch\n",
        "    \n",
        "    if eindex >= len(train_data):\n",
        "        batch = train_data[sindex:]\n",
        "        yield batch\n",
        "\n",
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
        "    return Variable(torch.LongTensor(idxs))\n",
        "\n",
        "def prepare_word(word, word2index):\n",
        "    return Variable(torch.LongTensor([word2index[word]]) if word2index.get(word) is not None else torch.LongTensor([word2index[\"<UNK>\"]]))\n",
        "\n",
        "def prepare_tag(tag,tag2index):\n",
        "    return Variable(torch.LongTensor([tag2index[tag]]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D_Zb-Y0HA3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ae03f94-9d61-4773-bb0f-a51888a570b5"
      },
      "source": [
        "# Separating training and testing data sets\n",
        "random.shuffle(windows)\n",
        "train_data = windows[:int(len(windows) * 0.7)]\n",
        "test_data = windows[int(len(windows) * 0.7):]\n",
        "\n",
        "len(train_data), len(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(900174, 385790)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA4y_1g95IV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining parameters for the model\n",
        "BATCH_SIZE = 1000\n",
        "EMBEDDING_SIZE = 50\n",
        "HIDDEN_SIZE = 300\n",
        "EPOCH = 3\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1DEOAO35IT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = WindowClassifier(len(word2index), EMBEDDING_SIZE, WINDOW_SIZE, HIDDEN_SIZE, len(tag2index))\n",
        "#model = model.cuda()\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUQxkJp25-DU",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N28Fk4-k5IQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "bda43f09-2856-4d4c-e161-7bbef6dcba04"
      },
      "source": [
        "for epoch in range(EPOCH):\n",
        "    losses = []\n",
        "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
        "        x,y=list(zip(*batch))\n",
        "        inputs = torch.cat([prepare_sequence(sent, word2index).view(1, -1) for sent in x])\n",
        "        targets = torch.cat([prepare_tag(tag, tag2index) for tag in y])\n",
        "        model.zero_grad()      \n",
        "        preds = model(inputs, is_training=True)\n",
        "        loss = loss_function(preds, targets)\n",
        "        losses.append(loss.data.tolist())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
        "            losses = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/3] mean_loss : 2.46\n",
            "[1/3] mean_loss : 0.01\n",
            "[2/3] mean_loss : 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fBRR1uO9MJB",
        "colab_type": "text"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3ptGqzk5IOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for_f1_score = []\n",
        "accuracy = 0\n",
        "for test in test_data:\n",
        "    x, y = test[0], test[1]\n",
        "    input_ = prepare_sequence(x, word2index).view(1, -1)\n",
        "\n",
        "    i = model(input_).max(1)[1]\n",
        "    pred = index2tag[i.data.tolist()[0]]\n",
        "    for_f1_score.append([pred, y])\n",
        "    if pred == y:\n",
        "        accuracy += 1\n",
        "\n",
        "print(accuracy/len(test_data) * 100)\n",
        "\n",
        "# Note that the accuracy seems high because the '[0]' label (which is the cause for the imbalance) is included in the results "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh4btwNq9rWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing '0' from the label because we are not interested in these words\n",
        "y_pred, y_test = list(zip(*for_f1_score))\n",
        "sorted_labels = sorted(\n",
        "    list(set(y_test) - {'[0]'}),\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lkq25f49rU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is because sklearn_crfsuite.metrics function flatten inputs\n",
        "y_pred = [[y] for y in y_pred] \n",
        "y_test = [[y] for y in y_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8JOL9MPMCO_",
        "colab_type": "text"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyVWvSKz9rTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "79fc4de7-20b4-413c-82ce-019f7422bae8"
      },
      "source": [
        "from sklearn_crfsuite import metrics\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels = sorted_labels, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       [CNP]      0.585     0.201     0.299       359\n",
            "       [EFD]      0.000     0.000     0.000        34\n",
            "       [GOV]      0.588     0.629     0.608       197\n",
            "       [JUR]      0.795     0.226     0.352       137\n",
            "       [LEG]      0.734     0.760     0.747       404\n",
            "       [PER]      0.000     0.000     0.000        17\n",
            "       [STD]      0.312     0.077     0.123       130\n",
            "       [TED]      0.000     0.000     0.000        26\n",
            "       [TIT]      0.869     0.574     0.691       197\n",
            "       [VAL]      1.000     0.038     0.074        26\n",
            "\n",
            "   micro avg      0.690     0.431     0.530      1527\n",
            "   macro avg      0.488     0.251     0.289      1527\n",
            "weighted avg      0.635     0.431     0.479      1527\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4DTQfEnuLvV",
        "colab_type": "text"
      },
      "source": [
        "#Packaging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQWRljBBuJZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import mlflow\n",
        "import mlflow.pyfunc\n",
        "\n",
        "state_dict_path = f'/content/drive/My Drive/AISC/nlp/state_dict.pt'\n",
        "torch.save(model.state_dict(), state_dict_path)\n",
        "\n",
        "artifacts = {'state_dict': state_dict_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2RPyOdauJYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
        "  # Load in the model and all required artifacts\n",
        "  # The context object is provided by the MLflow framework\n",
        "  # It will contain all of the artifacts specified above\n",
        "\n",
        "  def load_context(self, context):\n",
        "    import torch\n",
        "    import pickle\n",
        "    from model import WindowClassifier\n",
        "\n",
        "    # Initialize the model and load in the state dict\n",
        "    self.model = WindowClassifier()\n",
        "    self.model.load_state_dict(torch.load(context.artifacts[\"state_dict\"]))\n",
        "\n",
        "  # Create a predict function for our models\n",
        "  def predict(self, context, model_input):\n",
        "    pred = model(model_input).max(1)[1]\n",
        "    pred_labels = pred.data.tolist()[0]\n",
        "\n",
        "    return pred_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cicwd7qKuJUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "75234129-99e6-4d56-cfbb-b1d30462f7e0"
      },
      "source": [
        "mlflow.pyfunc.get_default_conda_env()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'channels': ['defaults', 'conda-forge'],\n",
              " 'dependencies': ['python=3.6.9',\n",
              "  'pip',\n",
              "  {'pip': ['mlflow', 'cloudpickle==1.3.0']}],\n",
              " 'name': 'mlflow-env'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhR0lLaXuJSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create our own conda environment\n",
        "conda_env = {\n",
        "    'channels': ['defaults', 'conda-forge'],\n",
        "    'dependencies': [f'python=3.6.9',\n",
        "                     f'pip=19.3.1'\n",
        "                     f'scikit-learn=0.23.3',\n",
        "                     {\n",
        "                         'pip':[f'mlflow=={mlflow.__version__}',\n",
        "                                'cloudpickle==1.3.0',\n",
        "                                'torch===1.5.1+cu101',\n",
        "                                'torchvision===0.6.1+cu101',\n",
        "                                'sklearn'\n",
        "                               ]\n",
        "                     }\n",
        "                    ],\n",
        "    'name': 'mlflow-env'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhmsFAIUuJM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bf91f305-4a50-4a86-a2f5-2cd15acb576d"
      },
      "source": [
        "# Location in our gdrive where we want the model to be saved\n",
        "mlflow_pyfunc_model_path = f\"/content/drive/My Drive/AISC/nlp/iuris_model\" \n",
        "\n",
        "# Package the model\n",
        "mlflow.pyfunc.save_model(path=mlflow_pyfunc_model_path,\n",
        "                         python_model=ModelWrapper(),\n",
        "                         artifacts=artifacts,\n",
        "                         conda_env=conda_env,\n",
        "                         code_path=['/content/drive/My Drive/AISC/nlp/model.py', '/content/drive/My Drive/AISC/nlp/metadata.txt'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdNuy7RTuJI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Bitop5uJEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDZDUPveuJAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}